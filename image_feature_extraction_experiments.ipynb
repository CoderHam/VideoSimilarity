{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    }
   ],
   "source": [
    "# import torch and other libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from IPython.display import display # to display images\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# other imports\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "resnet_model = models.resnet18(pretrained=True)\n",
    "alexnet_model = models.alexnet(pretrained=True)\n",
    "# Use the model object to select the desired layer\n",
    "resnet_layer = resnet_model._modules.get('avgpool')\n",
    "alexnet_layer = alexnet_model._modules.get('classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0, ceil_mode=False, count_include_pad=True)\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Dropout(p=0.5)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "resnet_model.eval()\n",
    "alexnet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image scaler to 224 x 224 pixels\n",
    "scaler = transforms.Scale((224, 224))\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vector(image_name, model):\n",
    "    # 1. Load the image with Pillow library\n",
    "    img = Image.open(image_name)\n",
    "    # 2. Create a PyTorch Variable with the transformed image\n",
    "    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))\n",
    "    if model == 'resnet':\n",
    "        # 3. Create a vector of zeros that will hold our feature vector\n",
    "        #    The 'avgpool' layer has an output size of 512\n",
    "        my_embedding = torch.zeros(512)\n",
    "        # 4. Define a function that will copy the output of a layer\n",
    "        def copy_data(m, i, o):\n",
    "            my_embedding.copy_(o.data)\n",
    "        # 5. Attach that function to our selected layer\n",
    "        h = resnet_layer.register_forward_hook(copy_data)\n",
    "        # 6. Run the model on our transformed image\n",
    "        resnet_model(t_img)\n",
    "        h.remove()\n",
    "    elif model == 'alexnet':\n",
    "        print('using alexnet...')\n",
    "        # 3. Create a vector of zeros that will hold our feature vector\n",
    "        #    The 'classifier' layer has an output size of 1000\n",
    "        my_embedding = torch.zeros(1000)\n",
    "        def copy_data(m, i, o):\n",
    "            my_embedding.copy_(o.data)\n",
    "        # 5. Attach that function to our selected layer\n",
    "        h = alexnet_layer.register_forward_hook(copy_data)\n",
    "        # 6. Run the model on our transformed image\n",
    "        alexnet_model(t_img)\n",
    "        h.remove()\n",
    "    # 8. Return the feature vector\n",
    "    return my_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/images/'\n",
    "img1 = DATA_PATH + 'golden1.jpg'\n",
    "# img2 = DATA_PATH + 'golden2.jpg'\n",
    "img2 = DATA_PATH + 'cat1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanbae/anaconda/envs/pytorch36/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trials: 10\n",
      "Image 1 Feature Vector Genration Time: 0.013s\n",
      "Image 2 Feature Vector Genration Time: 0.013s\n",
      "\n",
      "Corpus Size=3000000, Frames per Video=100\n",
      "Corpus Processing Time: 1083.333 hrs, or 45.139 days\n"
     ]
    }
   ],
   "source": [
    "# get feature vectors from resnet18\n",
    "n_trials = 10\n",
    "img1_resnet_times = []\n",
    "img2_resnet_times = []\n",
    "\n",
    "# get feature vectors n_trial times each and average\n",
    "for i in range(n_trials):\n",
    "    # image 1\n",
    "    t_start_img1 = time.time()\n",
    "    img1_vector = get_vector(img1, 'resnet')\n",
    "    img1_resnet_times.append(time.time() - t_start_img1)\n",
    "    # image 2\n",
    "    t_start_img2 = time.time()\n",
    "    img2_vector = get_vector(img2, 'resnet')\n",
    "    img2_resnet_times.append(time.time() - t_start_img2)\n",
    "\n",
    "# output results\n",
    "t_mean_img1_resnet = round(np.mean(img1_resnet_times)/n_trials, 3)\n",
    "t_mean_img2_resnet = round(np.mean(img2_resnet_times)/n_trials, 3)\n",
    "print('Number of Trials: {}'.format(n_trials))\n",
    "print('Image 1 Feature Vector Genration Time: {}s'.format(t_mean_img1_resnet))\n",
    "print('Image 2 Feature Vector Genration Time: {}s'.format(t_mean_img2_resnet))\n",
    "\n",
    "n_frames = 100\n",
    "n_corpus = 3e6\n",
    "t_corpus = round(t_mean_img1_resnet*n_frames*n_corpus/3600, 3)\n",
    "print('\\nCorpus Size={}, Frames per Video={}'.format(int(n_corpus), n_frames))\n",
    "print('Corpus Processing Time: {} hrs, or {} days'.format(t_corpus, round(t_corpus/24, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine similarity: \n",
      " 0.5390\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using PyTorch Cosine Similarity\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "cos_sim = cos(img1_vector.unsqueeze(0),\n",
    "              img2_vector.unsqueeze(0))\n",
    "print('\\nCosine similarity: {0}\\n'.format(cos_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet 18 experiments show that feature vector image generation takes aobut 1/100th of a second. This means over 45 days to process 3 million videos with 100 sampled images each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get feature vectors from alexnet\n",
    "# img1_vector_a = get_vector(img1, 'alexnet')\n",
    "# img2_vector_a = get_vector(img2, 'alexnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 512)\n"
     ]
    }
   ],
   "source": [
    "img1_vec = np.array(img1_vector)\n",
    "img2_vec = np.array(img2_vector)\n",
    "vec = np.array([img1_vec, img2_vec])\n",
    "print(vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch36]",
   "language": "python",
   "name": "conda-env-pytorch36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
