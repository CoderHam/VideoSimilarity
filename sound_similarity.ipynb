{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "from skimage import transform\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import knn_cnn_features\n",
    "import sound_similarity_inference\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.Popen(\"ffmpeg -i data/videos/animals/2.mp4 -f wav -vn data/tmp/animals_0.wav\", \\\n",
    "                     stdout=subprocess.PIPE, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "p_status = p.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there is an audio channel. If not, the output audio stream will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.Popen(\"ffprobe -i data/videos/animals/0.mp4 -show_streams -select_streams a -loglevel error\"\\\n",
    "                     , stdout=subprocess.PIPE, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "p_status = p.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No audio channel'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"No audio channel\" if output.decode()=='' else \"Audio channels present:\"+output.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ffprobe -i data/videos/animals/0.mp4 -show_streams -select_streams a -loglevel error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.Popen(\"ffprobe -i /home/hemant/Videos/Titans\\ -\\ Season\\ 1.mp4 -show_streams -select_streams a -loglevel error\", \\\n",
    "                     stdout=subprocess.PIPE, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "p_status = p.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Audio channels present:[STREAM]\\nindex=1\\ncodec_name=aac\\ncodec_long_name=AAC (Advanced Audio Coding)\\nprofile=LC\\ncodec_type=audio\\ncodec_time_base=1/44100\\ncodec_tag_string=mp4a\\ncodec_tag=0x6134706d\\nsample_fmt=fltp\\nsample_rate=44100\\nchannels=2\\nchannel_layout=stereo\\nbits_per_sample=0\\nid=N/A\\nr_frame_rate=0/0\\navg_frame_rate=0/0\\ntime_base=1/44100\\nstart_pts=0\\nstart_time=0.000000\\nduration_ts=114762752\\nduration=2602.329977\\nbit_rate=125588\\nmax_bit_rate=N/A\\nbits_per_raw_sample=N/A\\nnb_frames=112073\\nnb_read_frames=N/A\\nnb_read_packets=N/A\\nDISPOSITION:default=1\\nDISPOSITION:dub=0\\nDISPOSITION:original=0\\nDISPOSITION:comment=0\\nDISPOSITION:lyrics=0\\nDISPOSITION:karaoke=0\\nDISPOSITION:forced=0\\nDISPOSITION:hearing_impaired=0\\nDISPOSITION:visual_impaired=0\\nDISPOSITION:clean_effects=0\\nDISPOSITION:attached_pic=0\\nDISPOSITION:timed_thumbnails=0\\nTAG:creation_time=2018-12-03T23:09:27.000000Z\\nTAG:language=eng\\nTAG:handler_name=ISO Media file produced by Google Inc. Created on: 12/03/2018.\\n[/STREAM]\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"No audio channel\" if output.decode()=='' else \"Audio channels present:\"+output.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.Popen(\"ffmpeg -i /home/hemant/Videos/Titans\\ -\\ Season\\ 1.mp4 -f wav -vn data/tmp/animals_0.wav\", \\\n",
    "                     stdout=subprocess.PIPE, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "p_status = p.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file is huge. Taking a small chunk of it and delete the larger chunk\n",
    "!ffmpeg -t 30 -i data/tmp/animals_0.wav data/tmp/mini_0.wav -loglevel error && rm animals_0.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample script to create a log_mel_spectrogram\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "\n",
    "sample_rate=16000\n",
    "window_size=20\n",
    "step_size=10\n",
    "eps=1e-10\n",
    "rate, data = wavfile.read('data/tmp/mini_0.wav')\n",
    "if data.ndim > 1 : # ignore  channels 2+\n",
    "    data = data[:, 0]\n",
    "nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "freqs, times, spec = signal.spectrogram(data,fs=sample_rate,window='hann',nperseg=nperseg,noverlap=noverlap)\n",
    "log_specgram = np.log(spec.T.astype(np.float32) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8267, 161)\n"
     ]
    }
   ],
   "source": [
    "print(log_specgram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"audioset/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module **vggish_input** has a function **wavfile_to_examples** which returns the log_mel_spectrogram (after correction intro standard form) of the input **wav_file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vggish_input\n",
    "features_tensor = vggish_input.wavfile_to_examples(wav_file='data/tmp/mini_0.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "827 ms ± 20 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 96, 64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module **vggish_inference** has a function **main** which returns the vggish extracted embedding (from the log_mel_spectrogram that is in turn created by vggish_input) of the input **wav_file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from audioset/vggish_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import vggish_inference\n",
    "embedding_batch, postprocessed_batch = vggish_inference.main(wav_file='data/tmp/mini_0.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.94 s ± 6.51 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $31$ is the time in seconds of the audio stream and $128$ is the dimensionality of the extracted vggish embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read extracted audioset embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob \n",
    "tf.enable_eager_execution()\n",
    "\n",
    "files = sorted(glob.glob('audioset/data/audioset_v1_embeddings/bal_train/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTfRecords(tfrecords_filename, verbose = False, std_frames=10):\n",
    "    multiple_audio_embedding = []\n",
    "    labels = []\n",
    "\n",
    "    record_iterator = tf.python_io.tf_record_iterator(path=tfrecords_filename)\n",
    "    for string_record in record_iterator:\n",
    "        example = tf.train.SequenceExample.FromString(string_record)\n",
    "        label = np.array(example.context.feature['labels'].int64_list.value)\n",
    "        n_frames = len(example.feature_lists.feature_list['audio_embedding'].feature)\n",
    "        audio_embedding = []\n",
    "        for i in range(n_frames):\n",
    "            audio_embedding.append(tf.cast(tf.decode_raw(example.feature_lists.feature_list['audio_embedding']\\\n",
    "                        .feature[i].bytes_list.value[0],tf.uint8),tf.float32).numpy())\n",
    "        audio_embedding = np.array(audio_embedding)\n",
    "        \n",
    "        if n_frames==std_frames:\n",
    "            labels.append(label)\n",
    "            multiple_audio_embedding.append(audio_embedding)\n",
    "        if verbose:\n",
    "            print('labels: ' + str(label))\n",
    "            print(audio_embedding.shape)\n",
    "    return np.array(multiple_audio_embedding), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.1 ms, sys: 19 ms, total: 59.1 ms\n",
      "Wall time: 44.8 ms\n"
     ]
    }
   ],
   "source": [
    "%time multiple_audio_embedding, labels = readTfRecords(files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 10, 128), 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_audio_embedding.shape, len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4070"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4070/4070 [01:04<00:00, 63.43it/s]\n"
     ]
    }
   ],
   "source": [
    "all_audio_embedding = np.zeros((22176,10,128))\n",
    "all_labels = []\n",
    "pos = 0\n",
    "for file in tqdm(files):\n",
    "    multiple_audio_embedding, labels = readTfRecords(file)\n",
    "    length = len(labels)\n",
    "    if multiple_audio_embedding.shape!=(0,):\n",
    "        try:\n",
    "            all_audio_embedding[pos:(pos+length),] = np.array(multiple_audio_embedding)\n",
    "            all_labels = all_labels + labels\n",
    "            pos+=length\n",
    "        except:\n",
    "            print(multiple_audio_embedding.shape,file, length)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_audio_embedding = all_audio_embedding[:pos,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21782, 10, 128), 21782)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_audio_embedding.shape, len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"audio_embeddings\": shape (21782, 10, 128), type \"<f8\">"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audioset_h5f = h5py.File('audioset_balanced_features_vggish.h5', 'w')\n",
    "audioset_h5f.create_dataset('audio_embeddings', data=all_audio_embedding)\n",
    "audioset_h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('audioset_balanced_labels.npy', all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_labels = [item for sublist in all_labels for item in sublist]\n",
    "# from collections import Counter\n",
    "# Counter(flat_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN for Sound Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioset_h5f = h5py.File('audioset_balanced_features_vggish.h5', 'r')\n",
    "audio_embeddings = np.array(audioset_h5f['audio_embeddings'],dtype='float32')\n",
    "audioset_h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels = np.load('audioset_balanced_labels.npy')\n",
    "feature_labels = np.array([flab[0] for flab in feature_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21782, 10, 128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21782, 1280)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_audio_embeddings = audio_embeddings.reshape(audio_embeddings.shape[:-2] + (-1,))\n",
    "reshaped_audio_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 354 ms, sys: 221 ms, total: 576 ms\n",
      "Wall time: 575 ms\n"
     ]
    }
   ],
   "source": [
    "%time feature_indices = knn_cnn_features.run_knn_features(reshaped_audio_embeddings,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_videos = feature_labels[feature_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_accuracy(similar_videos, feature_labels, k=3):\n",
    "    accuracy = 0\n",
    "    for i, sim_vids in enumerate(similar_videos):\n",
    "        true_label = feature_labels[i]\n",
    "        for sim_vid in sim_vids:\n",
    "            pred_label = sim_vid\n",
    "            accuracy += np.sum(pred_label==true_label)/k\n",
    "    return accuracy/len(feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.47364796621067323\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",get_cls_accuracy(similar_videos,feature_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(feature_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we see a class-wise accuracy of $47\\%$ for the $448$ classes of audioset. Although the accuarcy is not high, this can be attributed to the following:\n",
    "\n",
    "    1. The larger number of class labels i.e. nearly 500.\n",
    "    2. The fact that we have considered only the first class label for each of the audio clips.\n",
    "    3. All votes are beening considered for calculating accuracy and not just majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of labels (if considering all - not just first)\n",
    "len(set([item for sublist in feature_labels for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.6 ms, sys: 96 ms, total: 110 ms\n",
      "Wall time: 109 ms\n"
     ]
    }
   ],
   "source": [
    "%time feature_indices = knn_cnn_features.run_knn_features(reshaped_audio_embeddings,\\\n",
    "                                                    test_vectors=reshaped_audio_embeddings[:10],k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script for getting K similar videos from a new video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sound_similarity_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from audioset/vggish_model.ckpt\n",
      "CPU times: user 1.28 s, sys: 245 ms, total: 1.53 s\n",
      "Wall time: 940 ms\n"
     ]
    }
   ],
   "source": [
    "%time audio_embedding = sound_similarity_inference.embedding_from_audio('data/audio/v_ApplyEyeMakeup_g01_c01.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embeddings, audio_labels, true_indices = sound_similarity_inference.load_sound_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 373 ms, total: 1.49 s\n",
      "Wall time: 807 ms\n"
     ]
    }
   ],
   "source": [
    "%time similar_indices = sound_similarity_inference.similar_sound_embedding(audio_embeddings[:1],k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sound_similarity_inference.label_from_index(similar_indices[0], true_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Smoke detector, smoke alarm' 'Alarm clock' 'Alarm clock' 'Boing'\n",
      " 'Emergency vehicle' 'Caw' 'Grunge' 'Music' 'Music' 'Silence'] \n",
      " ['Smoke detector, smoke alarm']\n"
     ]
    }
   ],
   "source": [
    "print(output[0],\"\\n\",output[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN with second wise similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioset_h5f = h5py.File('audioset_balanced_features_vggish.h5', 'r')\n",
    "audio_embeddings = np.array(audioset_h5f['audio_embeddings'],dtype='float32')\n",
    "audioset_h5f.close()\n",
    "    \n",
    "feature_labels = np.load('audioset_balanced_labels.npy')\n",
    "feature_labels = np.array([flab[0] for flab in feature_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_audio_embeddings = audio_embeddings.reshape(audio_embeddings.shape[:-3] + (-1,128))\n",
    "embedding_labels = np.array([i for i in feature_labels for _ in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((217820, 128), 217820)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_audio_embeddings.shape, len(embedding_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.39 s, sys: 2.34 s, total: 7.73 s\n",
      "Wall time: 7.74 s\n"
     ]
    }
   ],
   "source": [
    "%time feature_indices = knn_cnn_features.run_knn_features(merged_audio_embeddings,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6625822544591378\n"
     ]
    }
   ],
   "source": [
    "similar_videos = embedding_labels[feature_indices]\n",
    "print(\"Accuracy:\",get_cls_accuracy(similar_videos,embedding_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a train test split for KNN (70:30) to test accuracy for second-level KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(merged_audio_embeddings, embedding_labels, \\\n",
    "                                        test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 977 ms, sys: 479 ms, total: 1.46 s\n",
      "Wall time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "%time feature_indices = knn_cnn_features.run_knn_features(X_train,test_vectors=X_test,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.43782327915966457\n"
     ]
    }
   ],
   "source": [
    "similar_videos = y_train[feature_indices]\n",
    "print(\"Accuracy:\",get_cls_accuracy(similar_videos,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ordered_unique(listed):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in listed if not (x in seen or seen_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[137, 211]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ordered_unique(similar_videos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time for $1$ queries over $200,000$ frames of $128$ dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 ms ± 5.16 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit feature_indices = knn_cnn_features.run_knn_features(merged_audio_embeddings,\\\n",
    "                                                        test_vectors=X_test[:1],k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Sound features from UCF101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = sorted(glob.glob(\"data/UCF101/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_emb_from_vid(vid_path, vid_name):\n",
    "    sound_similarity_inference.extract_audio_from_video(vid_path)\n",
    "    audio_embedding = sound_similarity_inference.embedding_from_audio(\"data/audio/\"+vid_name+\".wav\")\n",
    "    return audio_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13320/13320 [2:08:45<00:00,  1.72it/s]  \n"
     ]
    }
   ],
   "source": [
    "for vid_path in tqdm(videos):\n",
    "    vid_name = vid_path.split('/')[-1].split('.')[0]\n",
    "    try:\n",
    "        audio_embedding = audio_emb_from_vid(vid_path, vid_name)\n",
    "        np.save('data/audio/'+vid_name+'.npy', audio_embedding)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = sorted(glob.glob(\"data/audio/*.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.33 % of the videos in the UCF dataset have an audio channel\n"
     ]
    }
   ],
   "source": [
    "print(\"%0.2f\" % (len(videos)*100/13320),\"% of the videos in the UCF dataset have an audio channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6837/6837 [00:20<00:00, 330.70it/s] \n"
     ]
    }
   ],
   "source": [
    "feature_vectors = None\n",
    "audio_labels = []\n",
    "for vid in tqdm(videos):\n",
    "    if type(feature_vectors) is not np.ndarray:\n",
    "        aud_emb = np.load(vid).astype('float32')\n",
    "        audio_labels = audio_labels + [vid]*aud_emb.shape[0]\n",
    "        feature_vectors = aud_emb\n",
    "    else:\n",
    "        aud_emb = np.load(vid).astype('float32')\n",
    "        audio_labels = audio_labels + [vid]*aud_emb.shape[0]\n",
    "        feature_vectors = np.vstack((feature_vectors, aud_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('audio_sec_UCF_vggish.h5', 'w')\n",
    "h5f.create_dataset('feature_vectors', data=feature_vectors)\n",
    "h5f.create_dataset('feature_labels', data=np.array(audio_labels, dtype='S'))\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second wise similarity UCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_file = h5py.File('audio_sec_UCF_vggish.h5', 'r')\n",
    "feature_labels = np.array([fl.decode() for fl in feature_file['feature_labels']])\n",
    "feature_vectors = np.array(feature_file['feature_vectors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479 ms ± 7.27 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit feature_indices = knn_cnn_features.run_knn_features(feature_vectors,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_accuracy_ucf(similar_videos, feature_labels, k=3):\n",
    "    accuracy = 0\n",
    "    for i, sim_vids in enumerate(similar_videos):\n",
    "        true_label = feature_labels[i].split('_')[1]\n",
    "        for sim_vid in sim_vids:\n",
    "            pred_label = sim_vid.split('_')[1]\n",
    "            accuracy += np.sum(pred_label==true_label)/k\n",
    "    return accuracy/len(feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7794387394018154\n"
     ]
    }
   ],
   "source": [
    "similar_videos = feature_labels[feature_indices]\n",
    "print(\"Accuracy:\",get_cls_accuracy_ucf(similar_videos,feature_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a train test split for KNN (70:30) to test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_vectors, feature_labels, \\\n",
    "                                        test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 ms ± 1.74 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit feature_indices = knn_cnn_features.run_knn_features(X_train,test_vectors=X_test,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.635109763142556\n"
     ]
    }
   ],
   "source": [
    "similar_videos = y_train[feature_indices]\n",
    "print(\"Accuracy:\",get_cls_accuracy_ucf(similar_videos,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
