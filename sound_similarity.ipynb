{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "from skimage import transform\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "import h5py\n",
    "import knn_cnn_features\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.Popen(\"ffmpeg -i data/videos/animals/2.mp4 -f wav -vn data/tmp/animals_0.wav\", \\\n",
    "                     stdout=subprocess.PIPE, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "p_status = p.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there is an audio channel. If not, the output audio stream will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.Popen(\"ffprobe -i data/videos/animals/0.mp4 -show_streams -select_streams a -loglevel error\"\\\n",
    "                     , stdout=subprocess.PIPE, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "p_status = p.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No audio channel'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"No audio channel\" if output.decode()=='' else \"Audio channels present:\"+output.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ffprobe -i data/videos/animals/0.mp4 -show_streams -select_streams a -loglevel error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.Popen(\"ffprobe -i /home/hemant/Videos/Titans\\ -\\ Season\\ 1.mp4 -show_streams -select_streams a -loglevel error\", \\\n",
    "                     stdout=subprocess.PIPE, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "p_status = p.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Audio channels present:[STREAM]\\nindex=1\\ncodec_name=aac\\ncodec_long_name=AAC (Advanced Audio Coding)\\nprofile=LC\\ncodec_type=audio\\ncodec_time_base=1/44100\\ncodec_tag_string=mp4a\\ncodec_tag=0x6134706d\\nsample_fmt=fltp\\nsample_rate=44100\\nchannels=2\\nchannel_layout=stereo\\nbits_per_sample=0\\nid=N/A\\nr_frame_rate=0/0\\navg_frame_rate=0/0\\ntime_base=1/44100\\nstart_pts=0\\nstart_time=0.000000\\nduration_ts=114762752\\nduration=2602.329977\\nbit_rate=125588\\nmax_bit_rate=N/A\\nbits_per_raw_sample=N/A\\nnb_frames=112073\\nnb_read_frames=N/A\\nnb_read_packets=N/A\\nDISPOSITION:default=1\\nDISPOSITION:dub=0\\nDISPOSITION:original=0\\nDISPOSITION:comment=0\\nDISPOSITION:lyrics=0\\nDISPOSITION:karaoke=0\\nDISPOSITION:forced=0\\nDISPOSITION:hearing_impaired=0\\nDISPOSITION:visual_impaired=0\\nDISPOSITION:clean_effects=0\\nDISPOSITION:attached_pic=0\\nDISPOSITION:timed_thumbnails=0\\nTAG:creation_time=2018-12-03T23:09:27.000000Z\\nTAG:language=eng\\nTAG:handler_name=ISO Media file produced by Google Inc. Created on: 12/03/2018.\\n[/STREAM]\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"No audio channel\" if output.decode()=='' else \"Audio channels present:\"+output.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.Popen(\"ffmpeg -i /home/hemant/Videos/Titans\\ -\\ Season\\ 1.mp4 -f wav -vn data/tmp/animals_0.wav\", \\\n",
    "                     stdout=subprocess.PIPE, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "p_status = p.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file is huge. Taking a small chunk of it and delete the larger chunk\n",
    "!ffmpeg -t 30 -i data/tmp/animals_0.wav data/tmp/mini_0.wav -loglevel error && rm animals_0.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample script to create a log_mel_spectrogram\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "\n",
    "sample_rate=16000\n",
    "window_size=20\n",
    "step_size=10\n",
    "eps=1e-10\n",
    "rate, data = wavfile.read('data/tmp/mini_0.wav')\n",
    "if data.ndim > 1 : # ignore  channels 2+\n",
    "    data = data[:, 0]\n",
    "nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "freqs, times, spec = signal.spectrogram(data,fs=sample_rate,window='hann',nperseg=nperseg,noverlap=noverlap)\n",
    "log_specgram = np.log(spec.T.astype(np.float32) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8267, 161)\n"
     ]
    }
   ],
   "source": [
    "print(log_specgram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"audioset/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module **vggish_input** has a function **wavfile_to_examples** which returns the log_mel_spectrogram (after correction intro standard form) of the input **wav_file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vggish_input\n",
    "features_tensor = vggish_input.wavfile_to_examples(wav_file='data/tmp/mini_0.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "827 ms ± 20 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 96, 64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module **vggish_inference** has a function **main** which returns the vggish extracted embedding (from the log_mel_spectrogram that is in turn created by vggish_input) of the input **wav_file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from audioset/vggish_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import vggish_inference\n",
    "embedding_batch, postprocessed_batch = vggish_inference.main(wav_file='data/tmp/mini_0.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.94 s ± 6.51 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $31$ is the time in seconds of the audio stream and $128$ is the dimensionality of the extracted vggish embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read extracted audioset embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob \n",
    "tf.enable_eager_execution()\n",
    "\n",
    "files = sorted(glob.glob('audioset/data/audioset_v1_embeddings/bal_train/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTfRecords(tfrecords_filename, verbose = False, std_frames=10):\n",
    "    multiple_audio_embedding = []\n",
    "    labels = []\n",
    "\n",
    "    record_iterator = tf.python_io.tf_record_iterator(path=tfrecords_filename)\n",
    "    for string_record in record_iterator:\n",
    "        example = tf.train.SequenceExample.FromString(string_record)\n",
    "        label = np.array(example.context.feature['labels'].int64_list.value)\n",
    "        n_frames = len(example.feature_lists.feature_list['audio_embedding'].feature)\n",
    "        audio_embedding = []\n",
    "        for i in range(n_frames):\n",
    "            audio_embedding.append(tf.cast(tf.decode_raw(example.feature_lists.feature_list['audio_embedding']\\\n",
    "                        .feature[i].bytes_list.value[0],tf.uint8),tf.float32).numpy())\n",
    "        audio_embedding = np.array(audio_embedding)\n",
    "        \n",
    "        if n_frames==std_frames:\n",
    "            labels.append(label)\n",
    "            multiple_audio_embedding.append(audio_embedding)\n",
    "        if verbose:\n",
    "            print('labels: ' + str(label))\n",
    "            print(audio_embedding.shape)\n",
    "    return np.array(multiple_audio_embedding), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.1 ms, sys: 19 ms, total: 59.1 ms\n",
      "Wall time: 44.8 ms\n"
     ]
    }
   ],
   "source": [
    "%time multiple_audio_embedding, labels = readTfRecords(files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 10, 128), 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_audio_embedding.shape, len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4070"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4070/4070 [01:04<00:00, 63.43it/s]\n"
     ]
    }
   ],
   "source": [
    "all_audio_embedding = np.zeros((22176,10,128))\n",
    "all_labels = []\n",
    "pos = 0\n",
    "for file in tqdm(files):\n",
    "    multiple_audio_embedding, labels = readTfRecords(file)\n",
    "    length = len(labels)\n",
    "    if multiple_audio_embedding.shape!=(0,):\n",
    "        try:\n",
    "            all_audio_embedding[pos:(pos+length),] = np.array(multiple_audio_embedding)\n",
    "            all_labels = all_labels + labels\n",
    "            pos+=length\n",
    "        except:\n",
    "            print(multiple_audio_embedding.shape,file, length)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_audio_embedding = all_audio_embedding[:pos,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21782, 10, 128), 21782)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_audio_embedding.shape, len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"audio_embeddings\": shape (21782, 10, 128), type \"<f8\">"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audioset_h5f = h5py.File('audioset_balanced_features_vggish.h5', 'w')\n",
    "audioset_h5f.create_dataset('audio_embeddings', data=all_audio_embedding)\n",
    "audioset_h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('audioset_balanced_labels.npy', all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_labels = [item for sublist in all_labels for item in sublist]\n",
    "# from collections import Counter\n",
    "# Counter(flat_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN for Sound Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioset_h5f = h5py.File('audioset_balanced_features_vggish.h5', 'r')\n",
    "audio_embeddings = np.array(audioset_h5f['audio_embeddings'],dtype='float32')\n",
    "audioset_h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels = np.load('audioset_balanced_labels.npy')\n",
    "feature_labels = np.array([flab[0] for flab in feature_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21782, 10, 128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21782, 1280)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_audio_embeddings = audio_embeddings.reshape(audio_embeddings.shape[:-2] + (-1,))\n",
    "reshaped_audio_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 354 ms, sys: 221 ms, total: 576 ms\n",
      "Wall time: 575 ms\n"
     ]
    }
   ],
   "source": [
    "%time feature_indices = knn_cnn_features.run_knn_features(reshaped_audio_embeddings,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_videos = feature_labels[feature_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_accuracy(similar_videos, feature_labels, k=3):\n",
    "    accuracy = 0\n",
    "    for i, sim_vids in enumerate(similar_videos):\n",
    "        true_label = feature_labels[i]\n",
    "        for sim_vid in sim_vids:\n",
    "            pred_label = sim_vid\n",
    "            accuracy += np.sum(pred_label==true_label)/k\n",
    "    return accuracy/len(feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.47364796621067323\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",get_cls_accuracy(similar_videos,feature_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(feature_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we see a class-wise accuracy of $47\\%$ for the $448$ classes of audioset. Although the accuarcy is not high, this can be attributed to the following:\n",
    "\n",
    "    1. The larger number of class labels i.e. nearly 500.\n",
    "    2. The fact that we have considered only the first class label for each of the audio clips.\n",
    "    3. All votes are beening considered for calculating accuracy and not just majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of labels (if considering all - not just first)\n",
    "len(set([item for sublist in feature_labels for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.6 ms, sys: 96 ms, total: 110 ms\n",
      "Wall time: 109 ms\n"
     ]
    }
   ],
   "source": [
    "%time feature_indices = knn_cnn_features.run_knn_features(reshaped_audio_embeddings,\\\n",
    "                                                    test_vectors=reshaped_audio_embeddings[:10],k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script for getting K similar videos from a new video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sound_similarity_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from audioset/vggish_model.ckpt\n",
      "CPU times: user 1.28 s, sys: 245 ms, total: 1.53 s\n",
      "Wall time: 940 ms\n"
     ]
    }
   ],
   "source": [
    "%time audio_embedding = sound_similarity_inference.embedding_from_audio('data/audio/v_ApplyEyeMakeup_g01_c01.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embeddings, audio_labels, true_indices = sound_similarity_inference.load_sound_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 373 ms, total: 1.49 s\n",
      "Wall time: 807 ms\n"
     ]
    }
   ],
   "source": [
    "%time similar_indices = sound_similarity_inference.similar_sound_embedding(audio_embeddings[:1],k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sound_similarity_inference.label_from_index(similar_indices[0], true_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Smoke detector, smoke alarm' 'Alarm clock' 'Alarm clock' 'Boing'\n",
      " 'Emergency vehicle' 'Caw' 'Grunge' 'Music' 'Music' 'Silence'] \n",
      " ['Smoke detector, smoke alarm']\n"
     ]
    }
   ],
   "source": [
    "print(output[0],\"\\n\",output[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN with second wise similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioset_h5f = h5py.File('audioset_balanced_features_vggish.h5', 'r')\n",
    "audio_embeddings = np.array(audioset_h5f['audio_embeddings'],dtype='float32')\n",
    "audioset_h5f.close()\n",
    "\n",
    "feature_labels = np.load('audioset_balanced_labels.npy')\n",
    "feature_labels = np.array([flab[0] for flab in feature_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_audio_embeddings = audio_embeddings.reshape(audio_embeddings.shape[:-3] + (-1,128))\n",
    "embedding_labels = np.array([i for i in feature_labels for _ in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((217820, 128), 217820)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_audio_embeddings.shape, len(embedding_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.39 s, sys: 2.34 s, total: 7.73 s\n",
      "Wall time: 7.74 s\n"
     ]
    }
   ],
   "source": [
    "%time feature_indices = knn_cnn_features.run_knn_features(merged_audio_embeddings,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6625822544591378\n"
     ]
    }
   ],
   "source": [
    "similar_videos = embedding_labels[feature_indices]\n",
    "print(\"Accuracy:\",get_cls_accuracy(similar_videos,embedding_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a train test split for KNN (70:30) to test accuracy for second-level KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(merged_audio_embeddings, embedding_labels, \\\n",
    "                                        test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 949 ms, sys: 521 ms, total: 1.47 s\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%time feature_indices = knn_cnn_features.run_knn_features(X_train,test_vectors=X_test,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.43782327915966457\n"
     ]
    }
   ],
   "source": [
    "similar_videos = y_train[feature_indices]\n",
    "print(\"Accuracy:\",get_cls_accuracy(similar_videos,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time for $1$ queries over $200,000$ frames of $128$ dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.4 ms ± 2.04 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit feature_indices = knn_cnn_features.run_knn_features(merged_audio_embeddings,\\\n",
    "                                                        test_vectors=X_test[:1],k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
